COMP 307 Assignment1 Part2
Decision Tree
"decisiontree.py" and "decisiontree.ipynb" are the program code for Decision Tree Classifier. 
I cannot see any output at home if I use command line arguments to compile the file.  
To make sure the whole file can be compiled, command line argument code has commented in the program file. 
So please put two files in the same directory with data files. 
They are python files, compile and execute the "decisiontree.py" under Python 3 or run "decisiontree.ipynb" by using Jupyther Notebook.

1. Compare the differences, baseline accuracy is higher:
Decision tree to training data:
112 instances
live: 91 correct out of 91
die: 21 correct out of 21
Accuracy: 100.00%,baseline accuracy: 100.00%

Decision tree to test data:
25 instances
live: 17 correct out of 20
die: 2 correct out of 5
Accuracy: 76.00%,baseline accuracy: 85.00%

Decision tree is shown at the end of this report and you can see it as an output in the program.

2.
Training set: hepatitis-training-run-0
Test set: hepatitis-test-run-0
30 instances
live: 21 correct out of 25
die: 3 correct out of 5
Accuracy: 80.00%,baseline accuracy: 84.00%

Training set: hepatitis-training-run-1
Test set: hepatitis-test-run-1
30 instances
live: 18 correct out of 24
die: 3 correct out of 6
Accuracy: 70.00%,baseline accuracy: 75.00%

Training set: hepatitis-training-run-2
Test set: hepatitis-test-run-2
30 instances
live: 20 correct out of 24
die: 0 correct out of 6
Accuracy: 66.67%,baseline accuracy: 83.33%

Training set: hepatitis-training-run-3
Test set: hepatitis-test-run-3
30 instances
live: 20 correct out of 23
die: 2 correct out of 7
Accuracy: 73.33%,baseline accuracy: 86.96%

Training set: hepatitis-training-run-4
Test set: hepatitis-test-run-4
30 instances
live: 22 correct out of 25
die: 2 correct out of 5
Accuracy: 80.00%,baseline accuracy: 88.00%

Training set: hepatitis-training-run-5
Test set: hepatitis-test-run-5
30 instances
live: 20 correct out of 23
die: 2 correct out of 7
Accuracy: 73.33%,baseline accuracy: 86.96%

Training set: hepatitis-training-run-6
Test set: hepatitis-test-run-6
30 instances
live: 22 correct out of 26
die: 0 correct out of 4
Accuracy: 73.33%,baseline accuracy: 84.62%

Training set: hepatitis-training-run-7
Test set: hepatitis-test-run-7
30 instances
live: 20 correct out of 23
die: 4 correct out of 7
Accuracy: 80.00%,baseline accuracy: 86.96%

Training set: hepatitis-training-run-8
Test set: hepatitis-test-run-8
30 instances
live: 14 correct out of 17
die: 5 correct out of 13
Accuracy: 63.33%,baseline accuracy: 82.35%

Training set: hepatitis-training-run-9
Test set: hepatitis-test-run-9
30 instances
live: 18 correct out of 22
die: 3 correct out of 8
Accuracy: 70.00%,baseline accuracy: 81.82%

Average accuracy: 73.00%

3.
①For each subtree that is not a leaf node in the decision tree, 
we try to replace it with a leaf node, and we replace the label of the leaf node 
with the label that has the most in the training samples covered by the subtree, 
thus generating a simplified decision tree, and then compare the performance of 
the two decision trees in the test data set. If the simplified decision tree has fewer errors in the test data set, 
then the subtree can be replaced with a leaf node. 
②Pruning makes the classifier lack of training and easy to miss the correlation between attributes.
It could brings the risk of under-fitting to the decision tree.
③Decision tree is a complex tree generated by fully considering all data points. 
There may be over-fitting. The more complex the decision tree is, the higher the degree of over-fitting will be.
Pruning can reduce the complexity of decision trees and the probability of over-fitting.

4.
Because this impurity measure we use is for 2 classes. 
(m instances class A, n instances class B, if m or n equals 0, then it is pure)
If there is three classes, it is not enough to detect only one class instances equal zero. 
There could be false positive values like other two classes instances are still messy. 
Thus, we need another formula to measure impurity for data which has more than two classes. 

